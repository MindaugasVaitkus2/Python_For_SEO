{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Extract The Text From Multiple Webpages In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing content analysis at scale, you'll need to automatically extract text content from web pages.\n",
    "\n",
    "In this article you'll learn how to extract the text content from single and multiple web pages using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.7/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (1.19.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from requests) (2.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.7/site-packages (2.3.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (50.3.0.post20201006)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.19.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
      "Requirement already satisfied: trafilatura in /opt/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests>=2.21.0; python_version > \"3.4\" in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (2.24.0)\n",
      "Requirement already satisfied: readability-lxml>=0.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (0.8.1)\n",
      "Requirement already satisfied: courlan>=0.2.3 in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (0.2.3)\n",
      "Requirement already satisfied: lxml>=4.6.1; python_version > \"3.4\" in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (4.6.1)\n",
      "Requirement already satisfied: justext>=2.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (2.2.0)\n",
      "Requirement already satisfied: htmldate>=0.7.2 in /opt/anaconda3/lib/python3.7/site-packages (from trafilatura) (0.7.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.21.0; python_version > \"3.4\"->trafilatura) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.21.0; python_version > \"3.4\"->trafilatura) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from requests>=2.21.0; python_version > \"3.4\"->trafilatura) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.21.0; python_version > \"3.4\"->trafilatura) (2020.6.20)\n",
      "Requirement already satisfied: cssselect in /opt/anaconda3/lib/python3.7/site-packages (from readability-lxml>=0.8.1->trafilatura) (1.1.0)\n",
      "Requirement already satisfied: tldextract in /opt/anaconda3/lib/python3.7/site-packages (from courlan>=0.2.3->trafilatura) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from htmldate>=0.7.2->trafilatura) (2.8.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract->courlan>=0.2.3->trafilatura) (1.5.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from tldextract->courlan>=0.2.3->trafilatura) (50.3.0.post20201006)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.8.1->htmldate>=0.7.2->trafilatura) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install numpy\n",
    "!pip install requests\n",
    "!pip install spacy\n",
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> NB: If you're writing this in a standard python file, you won't need to include the ! symbol. This is solely because this tutorial is written in a Jupyter Notebook. </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we'll break the problem down into several stages:\n",
    "\n",
    "1. Extract all of the HTML content using requests into a python dictionary.\n",
    "2. Pass every single HTML page to Trafilatura to parse the text content.\n",
    "3. Add error and exception handling so that if Trafilatura fails, we can still extract the content, albeit with a less accurate approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.models import MissingSchema\n",
    "import spacy\n",
    "import trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect The HTML Content From The Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://understandingdata.com/',\n",
    "      'https://sempioneer.com/',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for url in urls:\n",
    "    # 1. Obtain the response:\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    # 2. If the response content is 200 - Status Ok, Save The HTML Content:\n",
    "    if resp.status_code == 200:\n",
    "        data[url] = resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract The Text From A Single Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the all of the requests that had a status_code of 200, we can now apply several attempts to extract the text content from every request.\n",
    "\n",
    "Firstly we'll try to use [trafilatura](https://pypi.org/project/trafilatura/), however if this library is unable to extract the text, then we'll use BeautifulSoup4 as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautifulsoup_extract_text_fallback(response_content):\n",
    "    \n",
    "    '''\n",
    "    This is a fallback function, so that we can always return a value for text content.\n",
    "    Even for when both Trafilatura and BeautifulSoup are unable to extract the text from a \n",
    "    single URL.\n",
    "    '''\n",
    "    \n",
    "    # Create the beautifulsoup object:\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    \n",
    "    # Finding the text:\n",
    "    text = soup.find_all(text=True)\n",
    "    \n",
    "    # Remove unwanted tag elements:\n",
    "    cleaned_text = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head', \n",
    "        'input',\n",
    "        'script',\n",
    "        'style',]\n",
    "\n",
    "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
    "    # is NOT in the blacklist\n",
    "    for item in text:\n",
    "        if item.parent.name not in blacklist:\n",
    "            cleaned_text += '{} '.format(item)\n",
    "            \n",
    "    # Remove any tab separation and strip the text:\n",
    "    cleaned_text = cleaned_text.replace('\\t', '')\n",
    "    return cleaned_text.strip()\n",
    "    \n",
    "\n",
    "def extract_text_from_single_web_page(url):\n",
    "    \n",
    "    downloaded_url = trafilatura.fetch_url(url)\n",
    "    try:\n",
    "        a = trafilatura.extract(downloaded_url, json_output=True, with_metadata=True, include_comments = False,\n",
    "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    except AttributeError:\n",
    "        a = trafilatura.extract(downloaded_url, json_output=True, with_metadata=True,\n",
    "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    if a:\n",
    "        json_output = json.loads(a)\n",
    "        return json_output['text']\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            # We will only extract the text from successful requests:\n",
    "            if resp.status_code == 200:\n",
    "                return beautifulsoup_extract_text_fallback(resp.content)\n",
    "            else:\n",
    "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
    "                return np.nan\n",
    "        # Handling for any URLs that don't have the correct protocol\n",
    "        except MissingSchema:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_url = 'https://understandingdata.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_single_web_page(url=single_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm James Phoenix Data Scientist\n",
      "A digital marketer turned data scientist. I love data, statistics, marketing and want to help you use analytics to drive actionable change.\n",
      "- Python\n",
      "- SQL\n",
      "- GitHub\n",
      "- Machine Learning\n",
      "- Web Scraping\n",
      "- NLP\n",
      "- Data Visualisation\n",
      "- APIs\n",
      "- Communication\n",
      "Data Science Tactics To Improve Your Marketing\n",
      "Whether you're a data scientist looking for inspirational ideas to help your marketing team, or you're a digital marketer want to know how to use AI to improve your bottom line.\n",
      "Let's get started, together.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract The Text From Multiple Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a list comprehension with our single_extract text function to easily extract the text from many web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = urls + ['fake_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = [extract_text_from_single_web_page(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test better titles and meta descriptions to boost your click-through rate\n",
      "By blending data science, machine learning and statistics our platform not only provides you with A/B testing but also with a list of ML-predicted under-performing pages that you can choose to optimise.\n",
      "Enhance your organic search rankings by finding opportunistic keywords\n",
      "We’ll identify important words your pages could rank for, but don’t because they’re not currently within the <title tag> or article text.\n"
     ]
    }
   ],
   "source": [
    "print(text_content[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "print(text_content[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we've made sure that any URL that failed can easily be removed as we've returned np.nan (not a number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Our Raw Text From Multiple Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've successfully extracted the raw text documents, let's remove any web pages that failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_textual_content = [text for text in text_content if str(text) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you might want to clean the text for further analysis. For example, [tokenising the text content](https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/#:~:text=Tokenization%20is%20one%20of%20the,to%20working%20with%20text%20data.&text=Tokenization%20is%20essentially%20splitting%20a,smaller%20units%20are%20called%20tokens.) allows you to analyse the sentiment, the sentence structure, semantic dependencies and also the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a spacy token: Hello\n",
      "The estimated word count for this document is: 116.\n",
      "The estimated number of sentences in the document is: 13\n",
      "\n",
      "\n",
      "This is a spacy token: Test\n",
      "The estimated word count for this document is: 94.\n",
      "The estimated number of sentences in the document is: 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cleaned_text in cleaned_textual_content:\n",
    "    # 1. Create an NLP document with Spacy:\n",
    "    doc = nlp(cleaned_text)\n",
    "    # 2. Spacy has tokenised the text content:\n",
    "    print(f\"This is a spacy token: {doc[0]}\")\n",
    "    # 3. Extracting the word count per text document:\n",
    "    print(f\"The estimated word count for this document is: {len(doc)}.\")\n",
    "    # 4. Extracting the number of sentences:\n",
    "    print(f\"The estimated number of sentences in the document is: {len(list(doc.sents))}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you can now easily extract text content from either a single url or multiple urls. \n",
    "\n",
    "We've also included beautifulsoup as a failside/fallback function. This ensures that our code is less fragile and is able to withstand the following errors:\n",
    "\n",
    "- Invalid URLs.\n",
    "- URLs that had a failed status code (not 200).\n",
    "- Removing all URLs that we were unable to extract the text content from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
