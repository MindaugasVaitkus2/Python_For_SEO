{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping With Python And BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand the benefits and use cases of web scraping.\n",
    "- Learn how to parse the HTML content of a webpage using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/doc) to extract specific elements.\n",
    "- Learn how to scan the HTML for specific keywords.\n",
    "- Learn how to scrape multiple web pages.\n",
    "- Learn how to store your web scraped data into a pandas dataframe.\n",
    "- Learn how to save the web scraped data as a local .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following installations are for a Jupyter Notebook, however if you are using a command line then simply <strong> exclude the ! symbol </strong>\n",
    "\n",
    "~~~\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Learn Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning web scraping is a useful skill, whether you work as a programmer, marketer or analyst. Its a fantastic way for you to analyse websites. Web scraping should never replace a tool such as [ScreamingFrog](https://www.screamingfrog.co.uk/seo-spider/), however when you're creating data pipelines with Python or JavaScript scripts, then you'll likely want to write a custom scraper.\n",
    "\n",
    "Because what's the point of doing a website crawl if you only need a few pieces of information per page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have acquired advanced web scraping skills, you can:\n",
    "    \n",
    "- Accurately monitor your competitors.\n",
    "- Create data pipelines that push fresh HTML data into a data warehouse such as [BigQuery.](https://cloud.google.com/bigquery)\n",
    "- Allow you to blend it with other data sources such as [Google Search Console](https://search.google.com/search-console/about) or [Google Analytics](https://analytics.google.com/analytics/web/) data.\n",
    "- Create your own APIs for websites that don't publicly expose an API.\n",
    "\n",
    "There are many other uses for why [web scraping](https://understandingdata.com/what-is-web-scraping/) is a powerful skill to possess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly every website is different, this means it can be difficult to build a robust web scraper that will work on every website. You'll likely need to create unique selectors for each website which can be time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, your scripts are more likely to fail over time because websites change. Whenever a marketer, owner or developer makes changes to their website, it could lead to your script breaking. Therefore for larger proejcts its essential that you create a monitoring system so that you can fix these problems as they arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape A Single HTML Page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape a web page in python or any programming language, we will need to download the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library that we'll be using is [requests](https://requests.readthedocs.io/en/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.co.uk/jobs?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the status code is 200 (which means Ok), then we'll be able to access the web page. You can always check the status code with:\n",
    "\n",
    "~~~\n",
    "\n",
    "print(response.status_code)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the content of a request, simply use:\n",
    "    \n",
    "~~~\n",
    "\n",
    "response.content\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will store the HTML content as a stream of bytes:\n",
    "html_content = response.content\n",
    "\n",
    "# This will store the HTML content as a string:\n",
    "html_content_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the HTML Content to a Parser\n",
    "\n",
    "Simply downloading the HTML page is not enough, particularly if we would like to extract elements from it. Therefore we will use a python package called [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). BeautifulSoup provides us with a large amount of DOM (document object model) parsing methods. \n",
    "\n",
    "In order to parse the DOM of a page, simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that instead of a HTML bytes string, <strong> we have a BeautifulSoup object</strong>, that has many functions on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we'll be web scraping indeed and extracting job information from [Indeed.co.uk](https://www.indeed.co.uk/)\n",
    "\n",
    "* The job will be: data scientist.\n",
    "* The area will be london."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate The URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url = 'https://www.indeed.co.uk/jobs?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a lot of information inside of a URL. \n",
    "\n",
    "Its important for you to be able to identify the structure of URLs and to reverse engineer how they might have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <strong> The base URL </strong> means the path to the jobs functionality of the website which in this case is: https://www.index.co.uk/\n",
    "2. <strong> Query Parameters </strong> are a way for the jobs search to be dynamic, in the above example they are: \n",
    "<strong> ?q=data%20scientist&l=london&start=40&advn=2102673149993430&vjk=40339845379bc411'</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query parameters consist of:\n",
    "- The start of the query at q\n",
    "- A key and value for each query parameter (i.e. l = london or start=40)\n",
    "- A separator which is an ampersand symbol (&) that separates all of the key + value query parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually Inspect The Webpage In Google Chrome Dev Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping straight into coding, its worthwhile visually inspecting the HTML page content within your browser. This will give you a sense of how the website is constructed and what repeating patterns you can see within the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Chrome Developer tools is a free available tool that allows you to visually inspect the HTML code. \n",
    "\n",
    "Navigate to it by:\n",
    "\n",
    "1. Opening up Google Chrome.\n",
    "2. Right clicking on a webpage.\n",
    "3. Clicking inspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://sempioneer.com/wp-content/uploads/2020/11/google-chrome-dev-tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://sempioneer.com/wp-content/uploads/2020/11/google-chrome-dev-tools-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Element By HTML ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to select specific HTML elements by using the <strong> #id CSS selector.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Element By HTML Class Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can find elements by their class selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Extract Text From HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as selecting the entire HTML element, you can also easily extract the text using BeautifulSoup. \n",
    "\n",
    "Let's see how this might work whilst scraping a single job advertisement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_url = 'https://www.indeed.co.uk/viewjob?cmp=Crowd-Link-Consulting&t=Business+Intelligence+Engineer&jk=9129263166da1718&q=data+engineer&vjs=3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sempioneer.com/wp-content/uploads/2020/11/extracting-a-single-job-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting The Title Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's extract the title tag and then use .text to obtain the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can extract the first paragraph on the webpage, then get the text for that element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll want to store multiple elements, for example if there is a list of job advertisements on the same page. The following method will return a list of elements rather than just the first element:\n",
    "\n",
    "~~~\n",
    "\n",
    "soup.findAll(some_element)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to extract the text of every paragraph element, we could just do a list compehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to remove paragraph tags if they contain empty strings, by only including paragraphs which are truthy (don't have empty strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape Multiple HTML Pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to web scrape multiple pages, then we'll simply create a for loop and multiple beautifulsoup objects.\n",
    "\n",
    "The important things are:\n",
    "    \n",
    "- Have a results dictionary or list(s) that is outside of the loop.\n",
    "- Extract either the result or N/A or a NaN (not a number), this is especially important when you're using python lists as it ensures that all of your python lists will always be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://understandingdata.com/',\n",
    "       'https://understandingdata.com/about-me/',\n",
    "       'https://understandingdata.com/contact/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Scan HTML Content For Specific Keywords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularly in a marketing context, if one of your web pages is ranking for 5 keywords it would be beneficial to know:\n",
    "    \n",
    "- If every keyword was on a given HTML page.\n",
    "- If there were keywords on / missing from the HTML page.\n",
    "\n",
    "By writing a web scraper we can easily answer these questions at scale.\n",
    "\n",
    "----\n",
    "\n",
    "Let's say that our keyword is <strong> Understanding Data</strong>, we will normalise this to be lowercase with:\n",
    "\n",
    "~~~\n",
    "\n",
    "keyword = 'Understanding Data'.lower()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above, how easily it is to web scrape multiple pages and search the HTML content as well as the title tag.\n",
    "\n",
    "This can be extended to search many more HTML elements rather than just two.\n",
    "\n",
    "If we would like to do 30 or 50 it would be better to use this structure:\n",
    "    \n",
    "\n",
    "~~~\n",
    "\n",
    "for item_name, data in zip(['in_html', 'in_title'],[cleaned_html_text, title_tag]):\n",
    "    if keyword in data:\n",
    "        url_dict[url][item_name] = True\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Create A Pandas Dataframe From Web Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After web scraping and collecting your data from many web pages, it's ideal to store it within a pandas dataframe. From there you'll be able to push it directly to BigQuery or store it locally as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Save The Web Scraped Data To A .CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is inside of a pandas dataframe, we can easily save it with the following method:\n",
    "    \n",
    "~~~\n",
    "\n",
    "master_df.to_csv(file_name.csv)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this tutorial has sparked your curiosity with web scraping, I'd recommend reviewing the following resources to learn more:\n",
    "\n",
    "- [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/doc)\n",
    "- [Python Requests documentation](https://requests.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
