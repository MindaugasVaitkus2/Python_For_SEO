{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Web Scraping Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To understand the benefits of using async + await compared to simply web scraping with the [requests library.](https://requests.readthedocs.io/en/master/)\n",
    "- Learn how to create an asynchronous web scraper from scratch in pure python using [asyncio](https://docs.python.org/3/library/asyncio.html) and [aiohttp](https://docs.aiohttp.org/).\n",
    "- Practice downloading multiple webpages using Aiohttp + Asyncio and parsing HTML content per URL with [BeautifulSoup.](https://www.crummy.com/software/BeautifulSoup/doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python installations are for a Jupyter Notebook, however if you are using a command line then simply <strong> exclude the ! symbol </strong>\n",
    "\n",
    "~~~\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install aiohttp\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Note: The only reason why we use nest_asyncio is because this tutorial is written in a jupyter notebook, however if you wanted to write the same web scraper code in a python file, then you would'nt need to install or run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.7/site-packages (1.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Asychronous Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing synchronous web scrapers are easier and the code is less complex, however they're incredibly slow. \n",
    "\n",
    "This is because all of the requests must wait for the current request to finish one by one. <strong> There can only be one request running at a given time. </strong>\n",
    "\n",
    "\n",
    "In contrast, asynchronous web requests are able to execute without depending on previous requests within a queue or for loop. <strong> Asychronous requests happen simultaneously. </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asychronous vs synchronous computer science diagram](https://sempioneer.com/wp-content/uploads/2020/12/asychronous-vs-synchronous.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Is Asychronous Web Scraping Different To Using Python Requests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of thinking about creating a for loop with Xn requests, you need to think about creating an event loop. For example the environment for [NodeJS](https://nodejs.org/en/), by design executes in a single threaded event loop.\n",
    "\n",
    "However for Python, we will manually create an event loop with [asyncio.](https://docs.python.org/3/library/asyncio.html)\n",
    "\n",
    "Inside of your event loop, you can set a number of tasks to be completed and every task will be created and executed asychronously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape A Single Web Page Using Aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Content-type: text/html; charset=utf-8\n",
      "Body: <!doctype html> ...\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get('http://python.org') as response:\n",
    "\n",
    "            print(\"Status:\", response.status)\n",
    "            print(\"Content-type:\", response.headers['content-type'])\n",
    "\n",
    "            html = await response.text()\n",
    "            print(\"Body:\", html[:15], \"...\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we define a client session with aiohttp:\n",
    "\n",
    "~~~\n",
    "async with aiohttp.ClientSession() as session:\n",
    "\n",
    "~~~\n",
    "\n",
    "Then with our session, we execute a get response on a single URL:\n",
    "\n",
    "~~~\n",
    "async with session.get('http://python.org') as response:     \n",
    "        \n",
    " ~~~\n",
    " \n",
    " Thirdly, notice how we use the await keyword in front of response.text() like this:\n",
    " \n",
    "~~~\n",
    "html = await response.text()\n",
    "\n",
    "~~~\n",
    "\n",
    "Also note that every asychronous function starts with:\n",
    "\n",
    "~~~\n",
    "async def function_name\n",
    "~~~\n",
    "\n",
    "----\n",
    "\n",
    "Finally we run asyncio.run(main()), this creates an event loop and executes all tasks within it. \n",
    "\n",
    "After all of the tasks have been completed then the event loop is automatically destroyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Web Scrape Multiple Pages Using Aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping multiple pages with asyncio and aiohttp, we'll use the following pattern to create multiple tasks that will be simulataneously executed within an asyncio event loop:\n",
    "\n",
    "~~~\n",
    "tasks = []\n",
    "\n",
    "for url in urls:\n",
    "    tasks.append(some_function(session, url))\n",
    "    \n",
    "htmls = await asyncio.gather(*tasks)\n",
    "\n",
    "~~~\n",
    "\n",
    "To start with we create an empty list and then for every URL, we will attach an uncalled/uninvoked function, an AioHTTP session and the URL to the list.\n",
    "\n",
    "The asyncio.gather(*tasks), basically tells asyncio to keep running the event loop until all of these functions within the python have been completed. It will return a list that is the same length as the number of functions (unless one of the functions within the list returned zero results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create and execute multiple tasks, let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper(object):\n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "        # Global Place To Store The Data:\n",
    "        self.all_data  = []\n",
    "        self.master_dict = {}\n",
    "        # Run The Scraper:\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def fetch(self, session, url):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                text = await response.text()\n",
    "                return text, url\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "    async def main(self):\n",
    "        tasks = []\n",
    "        headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "        async with aiohttp.ClientSession(headers=headers) as session:\n",
    "            for url in self.urls:\n",
    "                tasks.append(self.fetch(session, url))\n",
    "\n",
    "            htmls = await asyncio.gather(*tasks)\n",
    "            self.all_data.extend(htmls)\n",
    "\n",
    "            # Storing the raw HTML data.\n",
    "            for html in htmls:\n",
    "                if html is not None:\n",
    "                    url = html[1]\n",
    "                    self.master_dict[url] = {'Raw Html': html[0]}\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a list of URLs for our scraper to get the data for:\n",
    "urls = ['https://understandingdata.com/', 'http://twitter.com/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the scraper class instance, this will automatically create a new event loop within the __init__ method:\n",
    "scraper = WebScraper(urls = urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Notice how we have a list length of 2:\n",
    "len(scraper.all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding HTML Parsing Logic To The Aiohttp Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as collecting the HTML response from multiple webpages, parsing the web page can be useful for SEO and HTML Content Analysis.\n",
    "\n",
    "Therefore let's create second function which will parse the HTML page and will extract the title tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "\n",
    "!pip install beautifulsoup\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper(object):\n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "        # Global Place To Store The Data:\n",
    "        self.all_data  = []\n",
    "        self.master_dict = {}\n",
    "        # Run The Scraper:\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def fetch(self, session, url):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                # 1. Extracting the Text:\n",
    "                text = await response.text()\n",
    "                # 2. Extracting the <title> </title> Tag:\n",
    "                title_tag = await self.extract_title_tag(text)\n",
    "                return text, url, title_tag\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    async def extract_title_tag(self, text):\n",
    "        try:\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            return soup.title\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "    async def main(self):\n",
    "        tasks = []\n",
    "        headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "        async with aiohttp.ClientSession(headers=headers) as session:\n",
    "            for url in self.urls:\n",
    "                tasks.append(self.fetch(session, url))\n",
    "\n",
    "            htmls = await asyncio.gather(*tasks)\n",
    "            self.all_data.extend(htmls)\n",
    "\n",
    "            # Storing the raw HTML data.\n",
    "            for html in htmls:\n",
    "                if html is not None:\n",
    "                    url = html[1]\n",
    "                    self.master_dict[url] = {'Raw Html': html[0], 'Title': html[2]}\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WebScraper(urls = urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>understandingdata.com | 520: Web server is returning an unknown error</title>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.master_dict['https://understandingdata.com/']['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous web scraping is more suitable when you have a larger number of URLs that need to be processed quickly. \n",
    "\n",
    "Also you can see how easy it is to add on a HTML parsing function with BeautifulSoup, allowing you to easily extract specific elements on a per URL basis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
