{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from errors import ValidationError, UnsupportedPlatformError\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sys import platform, exit\n",
    "\n",
    "# Loading Wrapper Classes\n",
    "from csv_parser import CsvParser\n",
    "from screaming_frog_automation import ScreamingFrogAnalyser\n",
    "\n",
    "# Utility Functions\n",
    "from utils import config_setup_check, dataframe_checker, dataframe_row_checker, YamlParser\n",
    "config = YamlParser()\n",
    "\n",
    "# Setup Variables - You will need to change these depending upon your Mac + Google Cloud Platform Setup!\n",
    "OUTPUTFOLDER = config.data['environment-variables']['OUTPUTFOLDER']\n",
    "SERVICE_ACCOUNT_KEY_LOCATION = config.data['environment-variables']['SERVICE_ACCOUNT_KEY_LOCATION']\n",
    "GOOGLE_CLOUD_PROJECT_ID = config.data['environment-variables']['GOOGLE_CLOUD_PROJECT_ID']\n",
    "GOOGLE_CLOUD_BIGQUERY_DATASET_ID = config.data['environment-variables']['GOOGLE_CLOUD_BIGQUERY_DATASET_ID']\n",
    "BIGQUERY_TABLE_ID_MAPPINGS = config.data['bigquery_table_id_mappings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY_LOCATION)\n",
    "client = bigquery.Client(credentials=credentials, project=GOOGLE_CLOUD_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bigquery_table = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Create A Single Tab with Specific Schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO(developer): Set table_id to the ID of the table to create.\n",
    "# # table_id = \"your-project.your_dataset.your_table_name\"\n",
    "\n",
    "# schema = [\n",
    "#     bigquery.SchemaField(\"full_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "#     bigquery.SchemaField(\"age\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "# ]\n",
    "\n",
    "# table = bigquery.Table(table_id, schema=schema)\n",
    "# table = client.create_table(table)  # Make an API request.\n",
    "# print(\n",
    "#     \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dynamic table generated from the parser class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Some Variables From The Mock-Test Data Folder:\n",
    "outputfolder = '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/Course/11_data_wrangling_screaming_frog/src/test_data'\n",
    "\n",
    "# Single File Paths:\n",
    "csv_single_file_path = [outputfolder + '/' +  '2020.07.04.19.48.59']\n",
    "seo_spider_single_file_path = [outputfolder + '/' + '2020-no-csv-exports-1']\n",
    "\n",
    "# Multiple File Paths:\n",
    "csv_multiple_file_paths = ['2020.07.04.19.48.59', '2020.07.04.19.49.38']; csv_multiple_file_paths = [outputfolder + '/' + item for item in csv_multiple_file_paths]\n",
    "seo_spider_multiple_file_paths = ['2020-no-csv-exports-1', '2020-no-csv-exports-2'];\n",
    "seo_spider_multiple_file_paths = [outputfolder + '/' + item for item in seo_spider_multiple_file_paths]\n",
    "\n",
    "website_urls = ['https://phoenixandpartners.co.uk/', 'https://phoenixandpartners.co.uk/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CsvParser(outputfolder=outputfolder,\n",
    "                  file_paths=csv_multiple_file_paths,\n",
    "                  website_urls=website_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any(dataframe_checker(parser)):\n",
    "        print('''Finished crawling and saved the output to your desired folder/folders. It's impossible to save to BigQuery because you have no .csv data.\n",
    "        Re-run the script with export_tabs, export_reports, or export_bulk_exports if you would like to upload to BigQuery!\n",
    "\n",
    "        Existing the program.\n",
    "        ''')\n",
    "        # exit() <-- Disabling this whilst running tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Data checking - Compile a list of dataframes that have both rows and columns:\n",
    "available_data = dataframe_row_checker(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigQueryAutomation():\n",
    "    def __init__(self, client, project_id, dataset_id,\n",
    "    available_data_dict, create_bigquery_table):\n",
    "        self.client = client\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.available_data_dict = available_data_dict\n",
    "        self.create_bigquery_table_boolean = create_bigquery_table\n",
    "        \n",
    "        # Automatically executed functions;\n",
    "        self._create_schema_dictionary()\n",
    "\n",
    "    # Helper Functions\n",
    "    def extract_schema_data(self, df, date_column_name):\n",
    "        table_schema = []\n",
    "        for name, dtype in zip(df.columns, df.dtypes):\n",
    "            if name == date_column_name:\n",
    "                table_schema.append('DATETIME')\n",
    "            elif dtype.name == 'object':\n",
    "                table_schema.append('STRING')\n",
    "            else:\n",
    "                table_schema.append(str(dtype).upper())\n",
    "        return table_schema\n",
    "\n",
    "    # Automatically create the required schema\n",
    "    def _create_schema_dictionary(self):\n",
    "        self.master_schema_data = []\n",
    "        for key, value in self.available_data_dict.items():\n",
    "            schema_dict = {}\n",
    "            schema_dict['name'] = key\n",
    "\n",
    "            # Extracting the column names:\n",
    "            # Replacing all () in pandas dataframes:\n",
    "            value.columns = value.columns.str.replace(\"[() ]\", \"_\")\n",
    "            bq_column_names = list(value.columns)\n",
    "            # Converting the date column:\n",
    "            value['Date'] = pd.to_datetime(value['Date'])\n",
    "            value['Date'] = value['Date'].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "            # Dynamically creating the SQL Schema From Two Lists (Column Names)\n",
    "            table_schema = self.extract_schema_data(value, 'Date')\n",
    "            schema_results = []\n",
    "            for name, schema in zip(bq_column_names, table_schema):\n",
    "                schema_results.append(bigquery.SchemaField(name, schema, mode='NULLABLE'))\n",
    "\n",
    "            schema_dict['data'] = value\n",
    "            schema_dict['schema'] = schema_results\n",
    "\n",
    "            # Save it to the schema data list\n",
    "            self.master_schema_data.append(schema_dict)\n",
    "            \n",
    "    def _create_single_bq_table(self, item):\n",
    "        name = item['name'].split('.')[0]\n",
    "        table_id = f\"{self.project_id}.{self.dataset_id}.{name}\"\n",
    "        table = bigquery.Table(table_id, schema=item['schema'])\n",
    "        table = self.client.create_table(table)  # Make an API request.\n",
    "\n",
    "    # Create BigQuery Tables\n",
    "    def _upload_single_bg_table(self, item):\n",
    "        # Assigning the references\n",
    "        table_id = item['name'].split('.')[0]\n",
    "        dataset_ref = self.client.dataset(self.dataset_id)\n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "        # Customise the Jobconfig setup\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "        job_config.schema = item['schema']\n",
    "        # Running the Job iside of a StringIO stream:\n",
    "        with io.StringIO(item['data'].to_json(orient=\"records\",lines=True)) as source_file:\n",
    "            job = self.client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "        job.result()\n",
    "    \n",
    "    # Execution Functions:\n",
    "    def create_all_bq_tables(self):\n",
    "        for result in self.master_schema_data:\n",
    "            self._create_single_bq_table(result)\n",
    "    \n",
    "    def upload_all_bq_tables(self):\n",
    "        for result in self.master_schema_data:\n",
    "            self._upload_single_bg_table(result)\n",
    "\n",
    "    # Push The Data To BigQuery If The BigQuery table is there it is an dataframe that is not empty\n",
    "    def automate_bq_reports(self):\n",
    "        if self.create_bigquery_table_boolean:\n",
    "            self.create_all_bq_tables()\n",
    "            self.upload_all_bq_tables()\n",
    "        else:\n",
    "            self.upload_all_bq_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check that all of the bg_automation keys have matches to the bgquery mappings dictionary\n",
    "checking_data = set(available_data.keys()) - set(BIGQUERY_TABLE_ID_MAPPINGS)\n",
    "\n",
    "if checking_data:\n",
    "    raise ValidationError(\"All of the BigQuery mappings haven't been matched against the available data\",\n",
    "                          '''BIGQUERY_TABLE_ID_MAPPINGS = config.data['bigquery_table_id_mappings']''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Change the name on the key to be the dictionary name instead of the standard one.\n",
    "if create_bigquery_table is False:\n",
    "    available_data = {BIGQUERY_TABLE_ID_MAPPINGS[key].replace(\"[() ]\", \"_\"): \n",
    "                      value for key, value in available_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Perform the BigQuery creation\n",
    "bg_automation = BigQueryAutomation(client=client, project_id=GOOGLE_CLOUD_PROJECT_ID,\n",
    "                                  dataset_id=GOOGLE_CLOUD_BIGQUERY_DATASET_ID, \n",
    "                                  available_data_dict=available_data,\n",
    "                                  create_bigquery_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_automation.automate_bq_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
